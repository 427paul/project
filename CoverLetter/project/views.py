import io
import json
import re
import time
import logging
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

import fitz  # PyMuPDF
import pandas as pd

from django.shortcuts import render
from django.views.decorators.csrf import csrf_exempt
from django.http import HttpRequest
from django.urls import reverse

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options as ChromeOptions
from bs4 import BeautifulSoup

from sentence_transformers import SentenceTransformer, util
from gradio_client import Client

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# =========================
# ì§ë¬´ ì¹´í…Œê³ ë¦¬ (Colab ì›ë³¸ ê·¸ëŒ€ë¡œ)
# =========================
JOB_CATEGORIES: Dict[str, List[str]] = {
    "ê¸°íšÂ·ì „ëµ": ["ê²½ì˜Â·ë¹„ì¦ˆë‹ˆìŠ¤ê¸°íš", "ì›¹ê¸°íš", "ë§ˆì¼€íŒ…ê¸°íš", "PLÂ·PMÂ·PO", "ì»¨ì„¤í„´íŠ¸", "CEOÂ·COOÂ·CTO", "AIê¸°íšì", "AIì‚¬ì—…ì „ëµ"],
    "ë²•ë¬´Â·ì‚¬ë¬´Â·ì´ë¬´": ["ê²½ì˜ì§€ì›", "ì‚¬ë¬´ë‹´ë‹¹ì", "ì´ë¬´", "ì‚¬ë¬´ë³´ì¡°", "ë²•ë¬´ë‹´ë‹¹ì", "ë¹„ì„œ", "ë³€í˜¸ì‚¬", "ë²•ë¬´ì‚¬", "ë³€ë¦¬ì‚¬", "ë…¸ë¬´ì‚¬", "AIìœ¤ë¦¬ì „ë¬¸ê°€"],
    "ì¸ì‚¬Â·HR": ["ì¸ì‚¬ë‹´ë‹¹ì", "HRDÂ·HRM", "ë…¸ë¬´ê´€ë¦¬ì", "ì¡ë§¤ë‹ˆì €", "í—¤ë“œí—Œí„°", "ì§ì—…ìƒë‹´ì‚¬"],
    "íšŒê³„Â·ì„¸ë¬´": ["íšŒê³„ë‹´ë‹¹ì", "ê²½ë¦¬", "ì„¸ë¬´ë‹´ë‹¹ì", "ì¬ë¬´ë‹´ë‹¹ì", "ê°ì‚¬", "IRÂ·ê³µì‹œ", "íšŒê³„ì‚¬", "ì„¸ë¬´ì‚¬", "ê´€ì„¸ì‚¬"],
    "ë§ˆì¼€íŒ…Â·ê´‘ê³ Â·MD": ["AE(ê´‘ê³ ê¸°íšì)", "ë¸Œëœë“œë§ˆì¼€í„°", "í¼í¬ë¨¼ìŠ¤ë§ˆì¼€í„°", "CRMë§ˆì¼€í„°", "ì˜¨ë¼ì¸ë§ˆì¼€í„°", "ì½˜í…ì¸ ë§ˆì¼€í„°", "í™ë³´", "ì„¤ë¬¸Â·ë¦¬ì„œì¹˜", "MD", "ì¹´í”¼ë¼ì´í„°", "í¬ë¦¬ì—ì´í‹°ë¸Œë””ë ‰í„°", "ì±„ë„ê´€ë¦¬ì", "ê·¸ë¡œìŠ¤í•´ì»¤"],
    "AIÂ·ê°œë°œÂ·ë°ì´í„°": ["ë°±ì—”ë“œê°œë°œì", "í”„ë¡ íŠ¸ì—”ë“œê°œë°œì", "ì›¹ê°œë°œì", "ì•±ê°œë°œì", "ì‹œìŠ¤í…œì—”ì§€ë‹ˆì–´", "ë„¤íŠ¸ì›Œí¬ì—”ì§€ë‹ˆì–´", "DBA", "ë°ì´í„°ì—”ì§€ë‹ˆì–´", "ë°ì´í„°ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸", "ë³´ì•ˆì—”ì§€ë‹ˆì–´", "ì†Œí”„íŠ¸ì›¨ì–´ê°œë°œì", "ê²Œì„ê°œë°œì", "í•˜ë“œì›¨ì–´ê°œë°œì", "AI/MLì—”ì§€ë‹ˆì–´", "ë¸”ë¡ì²´ì¸ê°œë°œì", "í´ë¼ìš°ë“œì—”ì§€ë‹ˆì–´", "ì›¹í¼ë¸”ë¦¬ì…”", "ITì»¨ì„¤íŒ…", "QA", "AI/MLì—°êµ¬ì›", "ë°ì´í„°ë¶„ì„ê°€", "ë°ì´í„°ë¼ë²¨ëŸ¬", "í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´", "AIë³´ì•ˆì „ë¬¸ê°€", "MLOpsì—”ì§€ë‹ˆì–´", "AIì„œë¹„ìŠ¤ê°œë°œì"],
    "ë””ìì¸": ["ê·¸ë˜í”½ë””ìì´ë„ˆ", "3Dë””ìì´ë„ˆ", "ì œí’ˆë””ìì´ë„ˆ", "ì‚°ì—…ë””ìì´ë„ˆ", "ê´‘ê³ ë””ìì´ë„ˆ", "ì‹œê°ë””ìì´ë„ˆ", "ì˜ìƒë””ìì´ë„ˆ", "ì›¹ë””ìì´ë„ˆ", "UIÂ·UXë””ìì´ë„ˆ", "íŒ¨ì…˜ë””ìì´ë„ˆ", "í¸ì§‘ë””ìì´ë„ˆ", "ì‹¤ë‚´ë””ìì´ë„ˆ", "ê³µê°„ë””ìì´ë„ˆ", "ìºë¦­í„°ë””ìì´ë„ˆ", "í™˜ê²½ë””ìì´ë„ˆ", "ì•„íŠ¸ë””ë ‰í„°", "ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´í„°"],
    "ë¬¼ë¥˜Â·ë¬´ì—­": ["ë¬¼ë¥˜ê´€ë¦¬ì", "êµ¬ë§¤ê´€ë¦¬ì", "ìì¬ê´€ë¦¬ì", "ìœ í†µê´€ë¦¬ì", "ë¬´ì—­ì‚¬ë¬´ì›"],
    "ìš´ì „Â·ìš´ì†¡Â·ë°°ì†¡": ["ë‚©í’ˆÂ·ë°°ì†¡ê¸°ì‚¬", "ë°°ë‹¬ê¸°ì‚¬", "ìˆ˜í–‰Â·ìš´ì „ê¸°ì‚¬", "í™”ë¬¼Â·ì¤‘ì¥ë¹„ê¸°ì‚¬", "ë²„ìŠ¤ê¸°ì‚¬", "íƒì‹œê¸°ì‚¬", "ì¡°ì¢…Â·ê¸°ê´€ì‚¬"],
    "ì˜ì—…": ["ì œí’ˆì˜ì—…", "ì„œë¹„ìŠ¤ì˜ì—…", "í•´ì™¸ì˜ì—…", "ê´‘ê³ ì˜ì—…", "ê¸ˆìœµì˜ì—…", "ë²•ì¸ì˜ì—…", "ITÂ·ê¸°ìˆ ì˜ì—…", "ì˜ì—…ê´€ë¦¬", "ì˜ì—…ì§€ì›"],
    "ê³ ê°ìƒë‹´Â·TM": ["ì¸ë°”ìš´ë“œìƒë‹´ì›", "ì•„ì›ƒë°”ìš´ë“œìƒë‹´ì›", "ê³ ê°ì„¼í„°ê´€ë¦¬ì"],
    "ê¸ˆìœµÂ·ë³´í—˜": ["ê¸ˆìœµì‚¬ë¬´", "ë³´í—˜ì„¤ê³„ì‚¬", "ì†í•´ì‚¬ì •ì‚¬", "ì‹¬ì‚¬", "ì€í–‰ì›Â·í…”ëŸ¬", "ê³„ë¦¬ì‚¬", "í€ë“œë§¤ë‹ˆì €", "ì• ë„ë¦¬ìŠ¤íŠ¸"],
    "ì‹Â·ìŒë£Œ": ["ìš”ë¦¬ì‚¬", "ì¡°ë¦¬ì‚¬", "ì œê³¼ì œë¹µì‚¬", "ë°”ë¦¬ìŠ¤íƒ€", "ì…°í”„Â·ì£¼ë°©ì¥", "ì¹´í˜Â·ë ˆìŠ¤í† ë‘ë§¤ë‹ˆì €", "í™€ì„œë²„", "ì£¼ë°©ë³´ì¡°", "ì†Œë¯ˆë¦¬ì—Â·ë°”í…ë”", "ì˜ì–‘ì‚¬", "ì‹í’ˆì—°êµ¬ì›", "í‘¸ë“œìŠ¤íƒ€ì¼ë¦¬ìŠ¤íŠ¸"],
    "ê³ ê°ì„œë¹„ìŠ¤Â·ë¦¬í…Œì¼": ["ì„¤ì¹˜Â·ìˆ˜ë¦¬ê¸°ì‚¬", "ì •ë¹„ê¸°ì‚¬", "í˜¸í…”ì¢…ì‚¬ì", "ì—¬í–‰ì—ì´ì „íŠ¸", "ë§¤ì¥ê´€ë¦¬ì", "ë·°í‹°Â·ë¯¸ìš©ì‚¬", "ì• ê²¬ë¯¸ìš©Â·í›ˆë ¨", "ì•ˆë‚´ë°ìŠ¤í¬Â·ë¦¬ì…‰ì…”ë‹ˆìŠ¤íŠ¸", "ê²½í˜¸Â·ê²½ë¹„", "ìš´ì˜ë³´ì¡°Â·ë§¤ë‹ˆì €", "ì´ë²¤íŠ¸Â·ì›¨ë”©í”Œë˜ë„ˆ", "ì£¼ì°¨Â·ì£¼ìœ ì›", "ìŠ¤íƒ€ì¼ë¦¬ìŠ¤íŠ¸", "ì¥ë¡€ì§€ë„ì‚¬", "ê°€ì‚¬ë„ìš°ë¯¸", "ìŠ¹ë¬´ì›", "í”Œë¡œë¦¬ìŠ¤íŠ¸"],
    "ì—”ì§€ë‹ˆì–´ë§Â·ì„¤ê³„": ["ì „ê¸°Â·ì „ìì—”ì§€ë‹ˆì–´", "ê¸°ê³„ì—”ì§€ë‹ˆì–´", "ì„¤ê³„ì—”ì§€ë‹ˆì–´", "ì„¤ë¹„ì—”ì§€ë‹ˆì–´", "ë°˜ë„ì²´ì—”ì§€ë‹ˆì–´", "í™”í•™ì—”ì§€ë‹ˆì–´", "ê³µì •ì—”ì§€ë‹ˆì–´", "í•˜ë“œì›¨ì–´ì—”ì§€ë‹ˆì–´", "í†µì‹ ì—”ì§€ë‹ˆì–´", "RFì—”ì§€ë‹ˆì–´", "í•„ë“œì—”ì§€ë‹ˆì–´", "R&DÂ·ì—°êµ¬ì›", "AIë¡œë´‡ì—”ì§€ë‹ˆì–´"],
    "ì œì¡°Â·ìƒì‚°": ["ìƒì‚°ì§ì¢…ì‚¬ì", "ìƒì‚°Â·ê³µì •ê´€ë¦¬ì", "í’ˆì§ˆê´€ë¦¬ì", "í¬ì¥Â·ê°€ê³µë‹´ë‹¹ì", "ê³µì¥ê´€ë¦¬ì", "ìš©ì ‘ì‚¬"],
    "êµìœ¡": ["ìœ ì¹˜ì›Â·ë³´ìœ¡êµì‚¬", "í•™êµÂ·íŠ¹ìˆ˜í•™êµêµì‚¬", "ëŒ€í•™êµìˆ˜Â·ê°•ì‚¬", "í•™ì›ê°•ì‚¬", "ì™¸êµ­ì–´ê°•ì‚¬", "ê¸°ìˆ Â·ì „ë¬¸ê°•ì‚¬", "í•™ìŠµì§€Â·ë°©ë¬¸êµì‚¬", "í•™ì›ìƒë‹´Â·ìš´ì˜", "êµì§ì›Â·ì¡°êµ", "êµì¬ê°œë°œÂ·êµìˆ˜ì„¤ê³„", "AIêµìœ¡ì»¨ì„¤í„´íŠ¸"],
    "ê±´ì¶•Â·ì‹œì„¤": ["ê±´ì¶•ê°€", "ê±´ì¶•ê¸°ì‚¬", "ì‹œê³µê¸°ì‚¬", "ì „ê¸°ê¸°ì‚¬", "í† ëª©ê¸°ì‚¬", "ì‹œì„¤ê´€ë¦¬ì", "í˜„ì¥ê´€ë¦¬ì", "ì•ˆì „ê´€ë¦¬ì", "ê³µë¬´", "ì†Œë°©ì„¤ë¹„", "í˜„ì¥ë³´ì¡°", "ê°ë¦¬ì›", "ë„ì‹œÂ·ì¡°ê²½ì„¤ê³„", "í™˜ê²½ê¸°ì‚¬", "ë¹„íŒŒê´´ê²€ì‚¬ì›", "ê³µì¸ì¤‘ê°œì‚¬", "ê°ì •í‰ê°€ì‚¬", "ë¶„ì–‘ë§¤ë‹ˆì €"],
    "ì˜ë£ŒÂ·ë°”ì´ì˜¤": ["ì˜ì‚¬", "í•œì˜ì‚¬", "ê°„í˜¸ì‚¬", "ê°„í˜¸ì¡°ë¬´ì‚¬", "ì•½ì‚¬Â·í•œì•½ì‚¬", "ì˜ë£Œê¸°ì‚¬", "ìˆ˜ì˜ì‚¬", "ìˆ˜ì˜í…Œí¬ë‹ˆì…˜", "ë³‘ì›ì½”ë””ë„¤ì´í„°", "ì›ë¬´í–‰ì •", "ê¸°íƒ€ì˜ë£Œì¢…ì‚¬ì", "ì˜ë£ŒÂ·ì•½ë¬´ë³´ì¡°", "ë°”ì´ì˜¤Â·ì œì•½ì—°êµ¬ì›", "ì„ìƒì—°êµ¬ì›"],
    "ë¯¸ë””ì–´Â·ë¬¸í™”Â·ìŠ¤í¬ì¸ ": ["PDÂ·ê°ë…", "í¬í† ê·¸ë˜í¼", "ì˜ìƒí¸ì§‘ì", "ì‚¬ìš´ë“œì—”ì§€ë‹ˆì–´", "ìŠ¤íƒœí”„", "ì¶œíŒÂ·í¸ì§‘", "ë°°ê¸‰Â·ì œì‘ì", "ì½˜í…ì¸ ì—ë””í„°", "í¬ë¦¬ì—ì´í„°", "ê¸°ì", "ì‘ê°€", "ì•„ë‚˜ìš´ì„œ", "ë¦¬í¬í„°Â·ì„±ìš°", "MCÂ·ì‡¼í˜¸ìŠ¤íŠ¸", "ëª¨ë¸", "ì—°ì˜ˆì¸Â·ë§¤ë‹ˆì €", "ì¸í”Œë£¨ì–¸ì„œ", "í†µë²ˆì—­ì‚¬", "íë ˆì´í„°", "ìŒë°˜ê¸°íš", "ìŠ¤í¬ì¸ ê°•ì‚¬", "AIì½˜í…ì¸ í¬ë¦¬ì—ì´í„°"],
    "ê³µê³µÂ·ë³µì§€": ["ì‚¬íšŒë³µì§€ì‚¬", "ìš”ì–‘ë³´í˜¸ì‚¬", "í™˜ê²½ë¯¸í™”ì›", "ë³´ê±´ê´€ë¦¬ì", "ì‚¬ì„œ", "ìì›ë´‰ì‚¬ì", "ë°©ì—­Â·ë°©ì¬ê¸°ì‚¬"]
}


# ===============
# ìœ í‹¸
# ===============
def get_category_code_from_selection(category_name: str) -> int:
    """
    Colab ë¡œì§: category_code = 10026 + arr1 - 1
    arr1ì€ 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì¹´í…Œê³ ë¦¬ ì¸ë±ìŠ¤. ì—¬ê¸°ì„œëŠ” 0-baseë¡œ ë³€í™˜í•´ì„œ 10026 + idx ì‚¬ìš©.
    """
    categories = list(JOB_CATEGORIES.keys())
    idx = categories.index(category_name)  # 0-based
    return 10026 + idx  # arr1-1 == idx


def extract_text_from_pdf_django(file) -> str:
    """
    Django UploadedFile â†’ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ (PyMuPDF)
    """
    # íŒŒì¼ì„ ë©”ëª¨ë¦¬ë¡œ ì½ì–´ì™€ streamìœ¼ë¡œ ì—´ê¸°
    content = file.read()
    doc = fitz.open(stream=content, filetype="pdf")
    text = ""
    for page in doc:
        text += page.get_text()
    doc.close()  # ë©”ëª¨ë¦¬ í•´ì œ
    return text


# ===============
# í¬ë¡¤ë§
# ===============
@dataclass
class PassResume:
    íšŒì‚¬: str
    ì—°ë„: str
    ì§ë¬´: str
    ì§ˆë¬¸: List[str]
    ë‹µë³€: List[str]
    ë§í¬: str


def get_selenium_driver() -> webdriver.Chrome:
    options = ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--window-size=1920,1080")
    options.add_argument("lang=ko_KR")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    # í”„ë¡œë•ì…˜ì—ì„œëŠ” chromedriver ê²½ë¡œ ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
    driver = webdriver.Chrome(options=options)
    return driver


def crawl_passassay_links(category_code: int, pages: int = 1) -> List[str]:
    """
    ëª©ë¡ í˜ì´ì§€ì—ì„œ í•©ê²©ìì†Œì„œ ìƒì„¸ ë§í¬ ìˆ˜ì§‘ (Colab ë¡œì§ ì¶©ì‹¤ ë°˜ì˜).
    """
    base_url = "https://www.jobkorea.co.kr/starter/PassAssay"
    links: List[str] = []

    driver = None
    try:
        driver = get_selenium_driver()
        for page in range(1, pages + 1):
            url = f"{base_url}?FavorCo_Stat=0&Pass_An_Stat=0&OrderBy=0&EduType=0&WorkType=0&schPart={category_code}&isSaved=1&Page={page}"
            driver.get(url)
            time.sleep(3)  # JS ë Œë”ë§ ëŒ€ê¸°

            soup = BeautifulSoup(driver.page_source, "html.parser")
            ul = soup.find("ul", class_="selfLists")
            if not ul:
                continue
            li_list = ul.find_all("li")
            for li in li_list:
                tag = li.find("a", class_="logo")
                if not tag:
                    continue
                href = tag.get("href", "")
                if href:
                    links.append("https://www.jobkorea.co.kr" + href)
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if driver:
            driver.quit()

    return links


def crawl_passassay_details(links: List[str]) -> List[PassResume]:
    """
    ìƒì„¸ í˜ì´ì§€ì—ì„œ íšŒì‚¬/ì—°ë„/ì§ë¬´/ì§ˆë¬¸/ë‹µë³€ ì¶”ì¶œ (Colab ë¡œì§ ì¶©ì‹¤ ë°˜ì˜).
    """
    data: List[PassResume] = []
    driver = None
    try:
        driver = get_selenium_driver()
        for link in links:
            try:
                driver.get(link)
                time.sleep(2)

                # ìƒë‹¨ ë°•ìŠ¤ í…ìŠ¤íŠ¸
                try:
                    assay_info = driver.find_element(By.CSS_SELECTOR, "div.selfTopBx").text
                    lines = assay_info.splitlines()

                    # íšŒì‚¬ / ì—°ë„ / ì§ë¬´
                    company = lines[0] if len(lines) > 0 else ""
                    title = lines[2] if len(lines) > 2 else ""  # ì˜ˆ: "2022ë…„ í•˜ë°˜ê¸° ì‹ ì… ë°±ì—”ë“œê°œë°œìí•©ê²©ìì†Œì„œ"
                    title_words = title.split()
                    year = title_words[0] if title_words else ""
                    apply_for = title_words[-1] if title_words else ""

                    # ì§ˆë¬¸
                    question_list: List[str] = []
                    question_elements = driver.find_elements(By.CSS_SELECTOR, "dt")
                    for elem in question_elements:
                        txt = elem.text
                        if txt.startswith("ì§ˆë¬¸"):
                            # "ì§ˆë¬¸ 1." ê°™ì€ ì ‘ë‘ ì œê±° ë¡œì§ ìœ ì§€
                            if len(txt) > 7:
                                question_list.append(txt[7:len(txt)-2] if txt.endswith("?") else txt[7:])

                    # ë‹µë³€ - dd show ì¶”ê°€ í›„ div.tx ìˆ˜ì§‘
                    script = """
                        const dds = document.querySelectorAll("dd");
                        dds.forEach(dd => dd.classList.add("show"));
                    """
                    driver.execute_script(script)
                    time.sleep(0.5)

                    answer_elements = driver.find_elements(By.CSS_SELECTOR, "div.tx")
                    answer_list = [elem.text.strip() for elem in answer_elements]
                    answer_list = answer_list[0:len(question_list)]

                    data.append(PassResume(
                        íšŒì‚¬=company,
                        ì—°ë„=year,
                        ì§ë¬´=apply_for[:-5] if apply_for and len(apply_for) > 5 else apply_for,  # "ë°±ì—”ë“œê°œë°œìí•©ê²©ìì†Œì„œ" â†’ "ë°±ì—”ë“œê°œë°œì"
                        ì§ˆë¬¸=question_list,
                        ë‹µë³€=answer_list,
                        ë§í¬=link
                    ))
                except Exception as e:
                    print(f"ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            except Exception as e:
                print(f"ë§í¬ {link} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        if driver:
            driver.quit()

    return data


# ===============
# ê°œì„ ëœ LLM: í•­ëª© ë¶„ë¥˜
# ===============

def create_robust_session() -> requests.Session:
    """
    íƒ€ì„ì•„ì›ƒê³¼ ì¬ì‹œë„ ì •ì±…ì´ ì ìš©ëœ ì„¸ì…˜ ìƒì„±
    """
    session = requests.Session()
    
    # ì¬ì‹œë„ ì „ëµ
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

def _parse_llm_response(output: str) -> pd.DataFrame:
    """
    LLM ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ ë° DataFrame ë³€í™˜
    """
    try:
        # ë‹¤ì–‘í•œ JSON ì½”ë“œë¸”ë¡ íŒ¨í„´ ì‹œë„
        patterns = [
            r"```json\s*(\[\s*{.*?}\s*\])\s*```",  # ê¸°ë³¸ íŒ¨í„´
            r"```\s*(\[\s*{.*?}\s*\])\s*```",      # json í‚¤ì›Œë“œ ì—†ëŠ” ê²½ìš°
            r"(\[\s*{.*?}\s*\])",                   # ì½”ë“œë¸”ë¡ ì—†ëŠ” ê²½ìš°
        ]
        
        for pattern in patterns:
            match = re.search(pattern, output, re.DOTALL)
            if match:
                json_text = match.group(1).strip()
                try:
                    user_data = json.loads(json_text)
                    df = pd.DataFrame(user_data)
                    
                    # ì»¬ëŸ¼ëª… ì •ê·œí™”
                    if "í•­ëª©" in df.columns and "ë‚´ìš©" in df.columns:
                        return df
                    elif "category" in df.columns and "content" in df.columns:
                        df = df.rename(columns={"category": "í•­ëª©", "content": "ë‚´ìš©"})
                        return df
                    elif len(df.columns) >= 2:
                        # ì²« ë‘ ì»¬ëŸ¼ì„ í•­ëª©/ë‚´ìš©ìœ¼ë¡œ ì‚¬ìš©
                        df.columns = ["í•­ëª©", "ë‚´ìš©"] + list(df.columns[2:])
                        return df
                        
                except json.JSONDecodeError as e:
                    logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨ (íŒ¨í„´ {pattern}): {e}")
                    continue
        
        logger.warning("JSON íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return pd.DataFrame()
        
    except Exception as e:
        logger.error(f"LLM ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

def _try_huggingface_classification(raw_text: str) -> pd.DataFrame:
    prompt = f"""ì•„ë˜ëŠ” ì´ë ¥ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ë‚´ìš©ë³„ë¡œ ì›ë³¸ ë‚´ìš©ì„ ìµœëŒ€í•œ ìœ ì§€í•œ ìƒíƒœë¡œ ë¶„ë¥˜í•´ì„œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
ê° í•­ëª©ì€ 'í•­ëª©ëª…'ê³¼ 'ë‚´ìš©'ìœ¼ë¡œ êµ¬ì„±í•˜ê³ , JSON í˜•íƒœë¡œ ì¶œë ¥í•˜ì„¸ìš”.

=== ì´ë ¥ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ ===
{raw_text}

ì¶œë ¥ í˜•ì‹:
```json
[
  {{"í•­ëª©": "ì´ë¦„", "ë‚´ìš©": "í™ê¸¸ë™"}},
  {{"í•­ëª©": "í•™ë ¥", "ë‚´ìš©": "â—‹â—‹ëŒ€í•™êµ ì»´í“¨í„°ê³µí•™ê³¼ ì¡¸ì—…"}},
  {{"í•­ëª©": "ê²½ë ¥", "ë‚´ìš©": "â–³â–³íšŒì‚¬ì—ì„œ 3ë…„ê°„ ë°±ì—”ë“œ ê°œë°œ ë‹´ë‹¹"}}
]
```"""
    try:
        client = Client("amd/gpt-oss-120b-chatbot")
        output = client.predict(prompt)
        logger.info(f"í…ìŠ¤íŠ¸ í•­ëª©ë³„ ë¶„ë¥˜ ì™„ë£Œ")
        #return output
    except Exception as e:
        logger.error(f"í…ìŠ¤íŠ¸ í•­ëª©ë³„ ë¶„ë¥˜ AI í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return str(e)
    try:
        return _parse_llm_response(output)
    except Exception as e:
        logger.error("ë¶„ë¥˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„°í™” ì‹¤íŒ¨")


def _fallback_rule_based_classification(raw_text: str) -> pd.DataFrame:
    """
    ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ (LLM ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ)
    """
    try:
        logger.info("ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì‹œì‘...")
        
        sections = []
        
        # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• 
        lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        
        current_section = None
        current_content = []
        
        # ì¼ë°˜ì ì¸ ì´ë ¥ì„œ ì„¹ì…˜ í‚¤ì›Œë“œ
        section_keywords = {
            "ì´ë¦„": ["ì´ë¦„", "ì„±ëª…", "name"],
            "ì—°ë½ì²˜": ["ì—°ë½ì²˜", "ì „í™”", "íœ´ëŒ€í°", "phone", "mobile", "email", "ì´ë©”ì¼"],
            "ì£¼ì†Œ": ["ì£¼ì†Œ", "ê±°ì£¼ì§€", "address", "ì†Œì¬ì§€"],
            "í•™ë ¥": ["í•™ë ¥", "ëŒ€í•™êµ", "ëŒ€í•™ì›", "ì¡¸ì—…", "education", "í•™ê³¼", "ì „ê³µ"],
            "ê²½ë ¥": ["ê²½ë ¥", "ê·¼ë¬´", "íšŒì‚¬", "career", "experience", "ë‹´ë‹¹", "ì—…ë¬´"],
            "ìê²©ì¦": ["ìê²©ì¦", "certificate", "certification", "ë©´í—ˆ", "í† ìµ", "í† í”Œ"],
            "ìˆ˜ìƒ": ["ìˆ˜ìƒ", "ìƒ", "award", "achievement", "ì…ìƒ"],
            "í”„ë¡œì íŠ¸": ["í”„ë¡œì íŠ¸", "project", "ê°œë°œ", "ì°¸ì—¬"],
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "skill", "ì–¸ì–´", "í”„ë¡œê·¸ë˜ë°", "framework"],
            "ìê¸°ì†Œê°œ": ["ìê¸°ì†Œê°œ", "ì†Œê°œ", "introduction", "ì§€ì›ë™ê¸°", "ë™ê¸°"],
            "ì„±ì¥ê³¼ì •": ["ì„±ì¥ê³¼ì •", "ì„±ì¥", "ì–´ë¦°ì‹œì ˆ", "ê°€ì •í™˜ê²½"],
            "ì„±ê²©ì˜ì¥ë‹¨ì ": ["ì„±ê²©", "ì¥ì ", "ë‹¨ì ", "personality", "strength", "weakness"],
            "ì…ì‚¬í›„í¬ë¶€": ["í¬ë¶€", "ê³„íš", "ëª©í‘œ", "vision", "future", "ì•ìœ¼ë¡œ"]
        }
        
        for line in lines:
            # í˜„ì¬ ë¼ì¸ì´ ìƒˆë¡œìš´ ì„¹ì…˜ ì‹œì‘ì¸ì§€ í™•ì¸
            found_section = None
            for section_name, keywords in section_keywords.items():
                if any(keyword in line.lower() for keyword in keywords):
                    # ê¸°ì¡´ ì„¹ì…˜ì´ ìˆë‹¤ë©´ ì €ì¥
                    if current_section and current_content:
                        content_text = ' '.join(current_content).strip()
                        if len(content_text) > 10:  # ë„ˆë¬´ ì§§ì€ ë‚´ìš©ì€ ì œì™¸
                            sections.append({
                                "í•­ëª©": current_section,
                                "ë‚´ìš©": content_text
                            })
                    
                    found_section = section_name
                    current_section = section_name
                    current_content = [line]
                    break
            
            if not found_section and current_section:
                # ê¸°ì¡´ ì„¹ì…˜ì— ë‚´ìš© ì¶”ê°€
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì €ì¥
        if current_section and current_content:
            content_text = ' '.join(current_content).strip()
            if len(content_text) > 10:
                sections.append({
                    "í•­ëª©": current_section,
                    "ë‚´ìš©": content_text
                })
        
        # ì„¹ì…˜ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ
        if len(sections) < 2:
            sections = [{"í•­ëª©": "ì „ì²´ë‚´ìš©", "ë‚´ìš©": raw_text}]
        
        logger.info(f"ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì™„ë£Œ: {len(sections)}ê°œ í•­ëª©")
        return pd.DataFrame(sections)
        
    except Exception as e:
        logger.error(f"ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def _minimal_fallback_classification(raw_text: str) -> pd.DataFrame:
    """
    ìµœì†Œí•œì˜ ë¶„ë¥˜ (ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ)
    """
    try:
        logger.info("ìµœì†Œí•œì˜ ë¶„ë¥˜ ì‹œì‘...")
        
        # í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
        max_chunk_size = 1000
        chunks = []
        
        if len(raw_text) <= max_chunk_size:
            chunks = [{"í•­ëª©": "ì „ì²´ë‚´ìš©", "ë‚´ìš©": raw_text}]
        else:
            # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
            paragraphs = [p.strip() for p in raw_text.split('\n\n') if p.strip()]
            
            if len(paragraphs) > 1:
                for i, para in enumerate(paragraphs[:5]):  # ìµœëŒ€ 5ê°œ ë¬¸ë‹¨
                    if len(para) > 50:  # ì¶©ë¶„íˆ ê¸´ ë¬¸ë‹¨ë§Œ
                        chunks.append({
                            "í•­ëª©": f"ì„¹ì…˜{i+1}",
                            "ë‚´ìš©": para
                        })
            
            if not chunks:
                # ë¬¸ë‹¨ ë¶„í•  ì‹¤íŒ¨ ì‹œ ê¸¸ì´ë¡œ ë¶„í• 
                chunk_size = len(raw_text) // 3
                chunks = [
                    {"í•­ëª©": "ì „ë°˜ë¶€", "ë‚´ìš©": raw_text[:chunk_size]},
                    {"í•­ëª©": "ì¤‘ë°˜ë¶€", "ë‚´ìš©": raw_text[chunk_size:chunk_size*2]},
                    {"í•­ëª©": "í›„ë°˜ë¶€", "ë‚´ìš©": raw_text[chunk_size*2:]}
                ]
        
        logger.info(f"ìµœì†Œ ë¶„ë¥˜ ì™„ë£Œ: {len(chunks)}ê°œ í•­ëª©")
        return pd.DataFrame(chunks)
        
    except Exception as e:
        logger.error(f"ìµœì†Œ ë¶„ë¥˜ë„ ì‹¤íŒ¨: {e}")
        # ì ˆëŒ€ ì‹¤íŒ¨í•˜ì§€ ì•ŠëŠ” ìµœí›„ì˜ ìˆ˜ë‹¨
        return pd.DataFrame([{"í•­ëª©": "ì „ì²´", "ë‚´ìš©": raw_text[:5000]}])

def classify_text_with_llm_to_dataframe(raw_text: str) -> pd.DataFrame:
    """
    ê°œì„ ëœ LLM í•­ëª© ë¶„ë¥˜ í•¨ìˆ˜ - ì—¬ëŸ¬ fallback ì˜µì…˜ê³¼ ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬
    """
    logger.info(f"í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‹œì‘ (ê¸¸ì´: {len(raw_text)}ì)")
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ë©´ ì²˜ë¦¬ ì‹œê°„ ì¦ê°€)
    if len(raw_text) > 10000:
        raw_text = raw_text[:10000] + "...(ìƒëµ)"
        logger.info("í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ 10000ìë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.")
    
    # 1ì°¨ ì‹œë„: Hugging Face Spaces
    try:
        logger.info("Hugging Face Spaces API í˜¸ì¶œ ì‹œì‘...")
        result_df = _try_huggingface_classification(raw_text)
        if not result_df.empty:
            logger.info("Hugging Face Spaces API ì„±ê³µ")
            return result_df
    except Exception as e:
        logger.warning(f"Hugging Face Spaces API ì‹¤íŒ¨: {e}")
    
    # 2ì°¨ ì‹œë„: ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
    try:
        logger.info("ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì‹œë„...")
        result_df = _fallback_rule_based_classification(raw_text)
        if not result_df.empty:
            logger.info("ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì„±ê³µ")
            return result_df
    except Exception as e:
        logger.warning(f"ë¡œì»¬ ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
    
    # 3ì°¨ ì‹œë„: ìµœì†Œí•œì˜ ë¶„ë¥˜
    logger.info("ìµœì†Œí•œì˜ ë¶„ë¥˜ë¡œ fallback")
    return _minimal_fallback_classification(raw_text)


# ===============
# ìœ ì‚¬ë„ ë§¤ì¹­ & í”¼ë“œë°± ìƒì„± (ìˆœì°¨ ì²˜ë¦¬ë¡œ ë³€ê²½)
# ===============

def build_user_qa(df: pd.DataFrame) -> List[Dict[str, str]]:
    """
    Colab ë¡œì§: ë‚´ìš© ê¸¸ì´ê°€ 100ì ì´ˆê³¼ì¸ í•­ëª©ë§Œ ì§ˆë¬¸/ë‹µë³€ìœ¼ë¡œ ê°„ì£¼.
    """
    user_qa = []
    for i in range(len(df)):
        content = str(df.iloc[i]["ë‚´ìš©"])
        if len(content) > 100:
            user_qa.append({
                "question": str(df.iloc[i]["í•­ëª©"]),
                "answer": content  # ì² ì ìˆ˜ì • (anwer -> answer)
            })
    return user_qa


def similarity_match_and_group(user_qa: List[Dict[str, str]], pass_data: List[PassResume], threshold: float = 0.5) -> pd.DataFrame:
    """
    - ko-srobertaë¡œ ì‚¬ìš©ì ì§ˆë¬¸ vs í•©ê²©ìì†Œì„œ ì§ˆë¬¸ ì„ë² ë”© í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    - threshold ì´ìƒ ì „ë¶€ ë§¤ì¹­, ì´í›„ ê·¸ë£¹ë³„ top3ë§Œ ë‚¨ê¹€ (Colab ë¡œì§).
    """
    try:
        model = SentenceTransformer('jhgan/ko-sroberta-multitask')

        user_questions = [qa["question"] for qa in user_qa]
        user_answers = [qa["answer"] for qa in user_qa]

        pass_questions: List[str] = []
        pass_answers: List[str] = []
        for d in pass_data:
            pass_questions.extend(d.ì§ˆë¬¸)
            pass_answers.extend(d.ë‹µë³€)

        if len(pass_questions) == 0:
            # í•©ê²© ìì†Œì„œ ì§ˆë¬¸ì´ ì—†ìœ¼ë©´ ë¹ˆ DF
            return pd.DataFrame(columns=["ì‚¬ìš©ì ì§ˆë¬¸", "ì‚¬ìš©ì ë‹µë³€", "í•©ê²© ì§ˆë¬¸", "í•©ê²© ë‹µë³€", "ìœ ì‚¬ë„"])

        user_embeds = model.encode(user_questions, convert_to_tensor=True)
        pass_embeds = model.encode(pass_questions, convert_to_tensor=True)
        cos_sim = util.cos_sim(user_embeds, pass_embeds)

        matched_data = []
        for i, user_q in enumerate(user_questions):
            sims = cos_sim[i]
            matched_indices = [j for j, score in enumerate(sims) if score >= threshold]

            if not matched_indices:
                matched_data.append({
                    "ì‚¬ìš©ì ì§ˆë¬¸": user_q,
                    "ì‚¬ìš©ì ë‹µë³€": user_answers[i],
                    "í•©ê²© ì§ˆë¬¸": None,
                    "í•©ê²© ë‹µë³€": None,
                    "ìœ ì‚¬ë„": None
                })
            else:
                for idx in matched_indices:
                    matched_data.append({
                        "ì‚¬ìš©ì ì§ˆë¬¸": user_q,
                        "ì‚¬ìš©ì ë‹µë³€": user_answers[i],
                        "í•©ê²© ì§ˆë¬¸": pass_questions[idx],
                        "í•©ê²© ë‹µë³€": pass_answers[idx],
                        "ìœ ì‚¬ë„": sims[idx].item()
                    })

        df_matched = pd.DataFrame(matched_data)

        if df_matched.empty or df_matched["ìœ ì‚¬ë„"].isna().all():
            return df_matched

        grouped = (
            df_matched
            .sort_values('ìœ ì‚¬ë„', ascending=False)
            .groupby(['ì‚¬ìš©ì ì§ˆë¬¸', 'ì‚¬ìš©ì ë‹µë³€'], group_keys=False)
            .apply(lambda g: g.nlargest(3, 'ìœ ì‚¬ë„'))  # ê° ê·¸ë£¹ë³„ ìƒìœ„ 3
            .groupby(['ì‚¬ìš©ì ì§ˆë¬¸', 'ì‚¬ìš©ì ë‹µë³€'])
            .agg({
                'í•©ê²© ì§ˆë¬¸': lambda x: list(x.dropna()),
                'í•©ê²© ë‹µë³€': lambda x: list(x.dropna()),
                'ìœ ì‚¬ë„': lambda x: list(x.dropna())
            })
            .reset_index()
        )
        return grouped
    except Exception as e:
        print(f"ìœ ì‚¬ë„ ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=["ì‚¬ìš©ì ì§ˆë¬¸", "ì‚¬ìš©ì ë‹µë³€", "í•©ê²© ì§ˆë¬¸", "í•©ê²© ë‹µë³€", "ìœ ì‚¬ë„"])


def generate_single_feedback_with_llm(user_question: str, user_answer: str, pass_questions: List[str], pass_answers: List[str]) -> str:
    """
    ë‹¨ì¼ ì§ˆë¬¸ì— ëŒ€í•´ í”¼ë“œë°±ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ (ìˆœì°¨ ì²˜ë¦¬ìš©) - ê°œì„ ëœ ë²„ì „
    """
    try:
        logger.info(f"í”¼ë“œë°± ìƒì„± ì‹œì‘: {user_question}")
        
        client = Client("amd/gpt-oss-120b-chatbot")
        
        # í•©ê²© ìì†Œì„œ ì—¬ëŸ¬ ê°œë¥¼ í•˜ë‚˜ì˜ ì°¸ê³  ìë£Œë¡œ ê²°í•©
        reference_examples = []
        for i, (pq, pa) in enumerate(zip(pass_questions, pass_answers)):
            reference_examples.append(f"[ì°¸ê³  ì˜ˆì‹œ {i+1}]\nì§ˆë¬¸: {pq}\në‹µë³€: {pa}")
        
        references_text = "\n\n".join(reference_examples)
        
        prompt = f"""
ì•„ë˜ëŠ” ì‚¬ìš©ì ìì†Œì„œ ì§ˆë¬¸ê³¼ ë‹µë³€, ê·¸ë¦¬ê³  ìœ ì‚¬í•œ í•©ê²© ìì†Œì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì€ í•©ê²© ìì†Œì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ìì†Œì„œì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì‚¬ìš©ì ë‹µë³€ê³¼ í•©ê²© ìì†Œì„œ ë‚´ìš©ì„ ë¹„êµí•˜ê³ , í•„ìš”í•˜ë©´ ëª©ë¡ í˜•íƒœì™€ ë¬¸ì¥ ë‹¨ìœ„ë¥¼ í˜¼í•©í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.
ì „ë¬¸ì ì´ê³  êµìœ¡ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìì†Œì„œ ì§ˆë¬¸: {user_question}
ì‚¬ìš©ì ìì†Œì„œ ë‹µë³€: {user_answer}

í•©ê²© ìì†Œì„œ ì§ˆë¬¸: {pq}
í•©ê²© ìì†Œì„œ ë‹µë³€: {pa}

ì‚¬ìš©ì ìì†Œì„œë¥¼ ë” ì˜ ë‹¤ë“¬ì„ ìˆ˜ ìˆë„ë¡ ë„ì›€ë§ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.

        """
        try:
            #client = Client("amd/gpt-oss-120b-chatbot")
            output = client.predict(prompt)
            logger.info(f"ë‹¨ì¼ ì§ˆë¬¸ í”¼ë“œë°± ìƒì„± ì™„ë£Œ : {user_question}")
            return output
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ì§ˆë¬¸ í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨: {e}")
            return f"'{user_question}' í•­ëª©ì˜ í”¼ë“œë°± ìƒì„± ì¤‘ íƒ€ì„ì•„ì›ƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        
    except Exception as e:
        logger.error(f"ë‹¨ì¼ í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return f"ì´ í•­ëª©ì— ëŒ€í•œ í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ê°œì„  ë°©í–¥: êµ¬ì²´ì ì¸ ê²½í—˜ ì„œìˆ , ìˆ˜ì¹˜í™”ëœ ì„±ê³¼, ì§ë¬´ ì—°ê´€ì„± ê°•í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”."


def generate_feedbacks_with_llm_sequential(grouped: pd.DataFrame, selected_category: str, selected_job: str):

    """
    ìˆœì°¨ ì²˜ë¦¬ë¡œ í”¼ë“œë°± ìƒì„± (íƒ€ì„ì•„ì›ƒ ë°©ì§€) - ê°œì„ ëœ ë²„ì „
    """
    # Space ì£¼ì†Œ (user/space_name)
    client = Client("amd/gpt-oss-120b-chatbot")

    final_feedbacks = []  # ê° ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… í”¼ë“œë°± ì €ì¥

    # ì „ì²´ ì‚¬ìš©ì ì§ˆë¬¸ ë°˜ë³µ
    for uq_idx in range(len(grouped['ì‚¬ìš©ì ì§ˆë¬¸'])):
        user_question = grouped['ì‚¬ìš©ì ì§ˆë¬¸'][uq_idx]
        user_answer = grouped['ì‚¬ìš©ì ë‹µë³€'][uq_idx]

        feedbacks = []  # í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ëª¨ë“  í”¼ë“œë°± ì €ì¥

        # í•´ë‹¹ ì‚¬ìš©ì ì§ˆë¬¸ì— ë§¤ì¹­ëœ ëª¨ë“  í•©ê²© ìì†Œì„œ ì§ˆë¬¸/ë‹µë³€ê³¼ ë¹„êµ
        for pq_idx in range(len(grouped['í•©ê²© ì§ˆë¬¸'][uq_idx])):
            pass_question = grouped['í•©ê²© ì§ˆë¬¸'][uq_idx][pq_idx]
            pass_answer = grouped['í•©ê²© ë‹µë³€'][uq_idx][pq_idx]

            prompt = f"""
            ì•„ë˜ëŠ” ì‚¬ìš©ì ìì†Œì„œ ì§ˆë¬¸ê³¼ ë‹µë³€, ê·¸ë¦¬ê³  ìœ ì‚¬í•œ í•©ê²© ìì†Œì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ì…ë‹ˆë‹¤.
            ë‹¹ì‹ ì€ í•©ê²© ìì†Œì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ìì†Œì„œì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ê±´ì„¤ì ì¸ í”¼ë“œë°±ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
            ì‚¬ìš©ì ë‹µë³€ê³¼ í•©ê²© ìì†Œì„œ ë‚´ìš©ì„ ë¹„êµí•˜ê³ , í•„ìš”í•˜ë©´ ëª©ë¡ í˜•íƒœì™€ ë¬¸ì¥ ë‹¨ìœ„ë¥¼ í˜¼í•©í•˜ì—¬ ì‘ì„±í•´ì£¼ì„¸ìš”.
            ì „ë¬¸ì ì´ê³  êµìœ¡ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
            ì‚¬ìš©ìëŠ” {selected_category}ì˜ {selected_job}ì— ì§€ì›í–ˆìŠµë‹ˆë‹¤.
            ì‚¬ìš©ì ìì†Œì„œ ì§ˆë¬¸: {user_question}
            ì‚¬ìš©ì ìì†Œì„œ ë‹µë³€: {user_answer}

            í•©ê²© ìì†Œì„œ ì§ˆë¬¸: {pass_question}
            í•©ê²© ìì†Œì„œ ë‹µë³€: {pass_answer}

            ì‚¬ìš©ì ìì†Œì„œë¥¼ ë” ì˜ ë‹¤ë“¬ì„ ìˆ˜ ìˆë„ë¡ ë„ì›€ë§ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.
            """

            output = client.predict(prompt)
            feedbacks.append(output)
            time.sleep(2)
        # ê° í”¼ë“œë°±ì„ ìš”ì•½
        summary_prompt = f"""
        ì•„ë˜ëŠ” ë™ì¼í•œ ì‚¬ìš©ì ìì†Œì„œ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ í•©ê²© ìì†Œì„œ ë‚´ìš©ì„ ì°¸ê³ í•´ ìƒì„±í•œ í”¼ë“œë°±ë“¤ì…ë‹ˆë‹¤.  
        ì´ í”¼ë“œë°±ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ **ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆë„ë¡**  
        â‘  êµ¬ì²´ì  ì‹¤í–‰ ì§€ì¹¨ í˜•íƒœì˜ "í•µì‹¬ ê°œì„  ë°©í–¥"ê³¼  
        â‘¡ í•­ëª©ë³„ë¡œ ì •ë¦¬ëœ "ì„¸ë¶€ ë³´ì™„ ì¡°ì–¸"ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.  

        - "í•µì‹¬ ê°œì„  ë°©í–¥"ì€ ë°˜ë“œì‹œ **ì˜ˆì‹œ ë¬¸ì¥**ì„ í¬í•¨í•´ ìš”ì•½ì ì´ì§€ë§Œ êµ¬ì²´ì ì¸ ê°€ì´ë“œë¼ì¸ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.  
        - "ì„¸ë¶€ ë³´ì™„ ì¡°ì–¸"ì€ í‘œ ëŒ€ì‹  **bullet list**ìœ¼ë¡œ ì •ë¦¬í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì…ë‹ˆë‹¤.  
        - ê° bulletì—ì„œë„ ** í‘œ ì‚¬ìš© ê¸ˆì§€ **, ë¦¬ìŠ¤íŠ¸ í˜•íƒœ í˜¹ì€ bullet listë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ë…ì„±ì„ í•´ì¹˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.
        - í‘œí˜„ì€ ì „ë¬¸ì ì´ê³  êµìœ¡ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.  
        {chr(10).join(feedbacks)}
        """

        summary_output = client.predict(summary_prompt)
        
        marker = "**ğŸ’¬ Response:**"
        if marker in summary_output:
            response_only = summary_output.split(marker, 1)[1].strip()
        else:
            response_only = summary_output  # markerê°€ ì—†ì„ ê²½ìš° ì „ì²´ ì‚¬ìš©

        #print(response_only)


        # ìµœì¢… ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        final_feedbacks.append({
            "question": user_question,
            "content": response_only
        })

    return final_feedbacks

# ===============
# ë©”ì¸ ë·° (ê°œì„ ëœ ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
# ===============

def index(request: HttpRequest):
    context: Dict[str, Any] = {
        "job_categories": json.dumps(JOB_CATEGORIES, ensure_ascii=False),  # í…œí”Œë¦¿ì—ì„œ JavaScriptë¡œ ì‚¬ìš©
        "job_categories_dict": JOB_CATEGORIES,  # í…œí”Œë¦¿ì—ì„œ Pythonìœ¼ë¡œ ì‚¬ìš©
        "selected_category": None,
        "selected_job": None,
        "results_ready": False,
        "final_feedbacks": [],
        "crawl_count": 0,
        "detail_count": 0,
        "matched_count": 0,
        "notes": []
    }

    if request.method == "POST":
        selected_category = request.POST.get("category")
        selected_job = request.POST.get("job")
        pdf_file = request.FILES.get("resume")

        context["selected_category"] = selected_category
        context["selected_job"] = selected_job

        if not selected_category or not selected_job or not pdf_file:
            context["notes"].append("ì¹´í…Œê³ ë¦¬, ì§ë¬´, PDF íŒŒì¼ì„ ëª¨ë‘ ì…ë ¥í•˜ì„¸ìš”.")
            return render(request, "index.html", context)

        try:
            # 1) category_code ê³„ì‚°
            category_code = get_category_code_from_selection(selected_category)
            context["notes"].append(f"ì„ íƒëœ ì¹´í…Œê³ ë¦¬ ì½”ë“œ: {category_code}")

            # 2) í•©ê²© ìì†Œì„œ ëª©ë¡ í¬ë¡¤ë§
            context["notes"].append("í•©ê²© ìì†Œì„œ ëª©ë¡ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            links = crawl_passassay_links(category_code=category_code, pages=1)
            context["crawl_count"] = len(links)
            
            if len(links) == 0:
                context["notes"].append("í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì—ì„œ í•©ê²© ìì†Œì„œ ëª©ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return render(request, "index.html", context)
            
            context["notes"].append(f"{len(links)}ê°œì˜ í•©ê²© ìì†Œì„œ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

            # 3) ìƒì„¸ í¬ë¡¤ë§
            context["notes"].append("í•©ê²© ìì†Œì„œ ìƒì„¸ ë‚´ìš©ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            pass_data_all = crawl_passassay_details(links)
            context["detail_count"] = len(pass_data_all)
            context["notes"].append(f"{len(pass_data_all)}ê°œì˜ í•©ê²© ìì†Œì„œ ìƒì„¸ ë‚´ìš©ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

            # 4) ì§ë¬´ í•„í„°
            pass_data = [d for d in pass_data_all if d.ì§ë¬´ == selected_job]
            if len(pass_data) < 3:
                context["notes"].append("ì„ íƒ ì§ë¬´ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í•©ê²© ìì†Œì„œê°€ ì ì–´ ì „ì²´ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                pass_data = pass_data_all

            # 5) ì‚¬ìš©ì PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
            context["notes"].append("PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            user_text = extract_text_from_pdf_django(pdf_file)
            context["notes"].append(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ê¸¸ì´: {len(user_text)}ì)")

            # 6) LLMìœ¼ë¡œ í•­ëª© ë¶„ë¥˜ (ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©)
            context["notes"].append("AIë¥¼ í†µí•´ ìì†Œì„œ ë‚´ìš©ì„ ë¶„ë¥˜í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            try:
                user_df = classify_text_with_llm_to_dataframe(user_text)
                context["notes"].append(f"{len(user_df)}ê°œì˜ í•­ëª©ìœ¼ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤.")
                
                # ë¶„ë¥˜ ê²°ê³¼ ë¡œê¹…
                if not user_df.empty:
                    logger.info(f"ë¶„ë¥˜ëœ í•­ëª©ë“¤: {list(user_df['í•­ëª©'])}")
                else:
                    logger.warning("ë¶„ë¥˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.error(f"í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
                context["notes"].append(f"í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
                # ê¸°ë³¸ ë¶„ë¥˜ë¡œ ëŒ€ì²´
                user_df = pd.DataFrame([{"í•­ëª©": "ì „ì²´ë‚´ìš©", "ë‚´ìš©": user_text[:2000]}])
                context["notes"].append("ê¸°ë³¸ ë¶„ë¥˜ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

            # 7) ì‚¬ìš©ì ì§ˆë¬¸-ë‹µë³€ë§Œ ì¶”ì¶œ(ë‚´ìš© ê¸¸ì´>100)
            user_qa = build_user_qa(user_df)
            context["notes"].append(f"{len(user_qa)}ê°œì˜ ì£¼ìš” í•­ëª©ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")

            if len(user_qa) == 0:
                context["notes"].append("ìì†Œì„œì—ì„œ ì¶©ë¶„íˆ ê¸´ í•­ëª©(>100ì)ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                user_qa = [{"question": "ì „ì²´ ë‚´ìš©", "answer": user_text[:1000]}]

            # 8) ìœ ì‚¬ë„ ë§¤ì¹­ & ê·¸ë£¹í™”
            context["notes"].append("í•©ê²© ìì†Œì„œì™€ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            ## 1ì°¨ ì‹œë„ : pass_dataë¡œ ë§¤ì¹­
            grouped_df = similarity_match_and_group(user_qa, pass_data, threshold=0.5)
            
            ## ì‚¬ìš©ì ì§ˆë¬¸ë³„ë¡œ "í•©ê²© ë‹µë³€" ê°œìˆ˜ í™•ì¸
            need_expand = any(len(ans_list) < 3 for ans_list in grouped_df["í•©ê²© ë‹µë³€"])
            ## 2ì°¨ ì‹œë„: ë¶€ì¡±í•œ ê²½ìš° ì „ì²´ pass_data_allë¡œ ë‹¤ì‹œ ë§¤ì¹­
            if need_expand:
                context["notes"].append("ì¼ë¶€ ì§ˆë¬¸ì—ì„œ ë§¤ì¹­ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ì „ì²´ í•©ê²© ìì†Œì„œë¥¼ ë‹¤ì‹œ íƒìƒ‰í•©ë‹ˆë‹¤...")
                grouped_df = similarity_match_and_group(user_qa, pass_data_all, threshold=0.5)
            context["matched_count"] = len(grouped_df) if not grouped_df.empty else 0
            context["notes"].append(f"{context['matched_count']}ê°œì˜ í•­ëª©ì—ì„œ ìœ ì‚¬í•œ í•©ê²© ìì†Œì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

            # 9) ìˆœì°¨ ì²˜ë¦¬ë¡œ í”¼ë“œë°± ìƒì„±
            context["notes"].append("AIë¥¼ í†µí•´ ë§ì¶¤í˜• í”¼ë“œë°±ì„ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            if grouped_df.empty or len(grouped_df) == 0:
                # ë§¤ì¹­ ì—†ìœ¼ë©´ ê¸°ë³¸ í”¼ë“œë°±
                context["final_feedbacks"] = [{
                    "question": user_qa[0]["question"] if user_qa else "ì „ì²´ ë‚´ìš©",
                    "content": """í•©ê²© ìì†Œì„œì™€ì˜ ì§ì ‘ì  ìœ ì‚¬ í•­ëª©ì´ ì ì–´ ì¼ë°˜ì ì¸ ê°œì„  ë°©í–¥ì„ ì œì•ˆë“œë¦½ë‹ˆë‹¤:

1. **êµ¬ì²´ì ì¸ ê²½í—˜ ì„œìˆ **: ë‹¨ìˆœí•œ ë‚˜ì—´ë³´ë‹¤ëŠ” êµ¬ì²´ì ì¸ ìƒí™©, í–‰ë™, ê²°ê³¼(STAR ê¸°ë²•)ë¥¼ í™œìš©í•˜ì„¸ìš”.

2. **ìˆ˜ì¹˜í™”ëœ ì„±ê³¼**: ê°€ëŠ¥í•œ ëª¨ë“  ì„±ê³¼ë¥¼ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•´ë³´ì„¸ìš”. (ì˜ˆ: "ë§¤ì¶œ 20% ì¦ê°€", "íš¨ìœ¨ì„± 30% ê°œì„ " ë“±)

3. **ì§ë¬´ ì—°ê´€ì„± ê°•í™”**: ì§€ì›í•˜ëŠ” ì§ë¬´ì™€ ê´€ë ¨ëœ ì—­ëŸ‰ê³¼ ê²½í—˜ì„ ëª…í™•íˆ ì—°ê²°ì§€ì–´ ì„œìˆ í•˜ì„¸ìš”.

4. **ì°¨ë³„í™” ìš”ì†Œ**: ë‹¤ë¥¸ ì§€ì›ìì™€ êµ¬ë³„ë˜ëŠ” ë³¸ì¸ë§Œì˜ ë…íŠ¹í•œ ê²½í—˜ì´ë‚˜ ê´€ì ì„ í¬í•¨í•˜ì„¸ìš”.

5. **ë…¼ë¦¬ì  êµ¬ì„±**: ê° ë¬¸ë‹¨ì´ ëª…í™•í•œ ì£¼ì œë¥¼ ê°€ì§€ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì—°ê²°ë˜ë„ë¡ êµ¬ì„±í•˜ì„¸ìš”."""
                }]
            else:
                # ìˆœì°¨ ì²˜ë¦¬ë¡œ í”¼ë“œë°± ìƒì„±
                try:
                    final_feedbacks = generate_feedbacks_with_llm_sequential(grouped_df, selected_category, selected_job)
                    context["final_feedbacks"] = final_feedbacks if final_feedbacks else [{
                        "question": "í”¼ë“œë°± ìƒì„±",
                        "content": "í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                    }]
                except Exception as e:
                    logger.error(f"í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                    context["final_feedbacks"] = [{
                        "question": "í”¼ë“œë°± ìƒì„± ì˜¤ë¥˜",
                        "content": f"í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}. ì¼ë°˜ì ì¸ ê°œì„  ë°©í–¥ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”."
                    }]

            context["results_ready"] = True
            context["notes"].append("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            context["notes"].append(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            return render(request, "index.html", context)

        return render(request, "index.html", context)

    # GET ìš”ì²­
    return render(request, "index.html", context)

